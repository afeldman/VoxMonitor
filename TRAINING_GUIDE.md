# ğŸš€ VoxMonitor Training - Implementation Summary

## Status: âœ… READY FOR 1-EPOCH TEST

Das VoxMonitor-Projekt wurde vollstÃ¤ndig mit DeepSuite integriert und kann jetzt mit den lokalen Soundwell-Daten trainiert werden.

---

## ğŸ“‹ DurchgefÃ¼hrte Implementierungen

### 1. **VoxMonitorTrainer** (train.py)
- âœ… Erweitert DeepSuite's BaseTrainer
- âœ… VollstÃ¤ndige Initialisierung mit super().__init__()
- âœ… Automatische ONNX-Export-UnterstÃ¼tzung
- âœ… MLflow-Integration
- âœ… Checkpoint-Management

### 2. **SoundwelDataset Enhancement** (data.py)
- âœ… Flexible CSV-Datei-UnterstÃ¼tzung
- âœ… Pre-loaded DataFrame-UnterstÃ¼tzung
- âœ… Automatische Spalten-Name-Zuordnung
- âœ… Audio-Datei-Loading mit torchaudio
- âœ… Mel-Spektrogramm-Extraktion

**Neue Parameter:**
```python
SoundwelDataset(
    root_dir="/Volumes/.../Soundwel",
    csv_path="/Volumes/.../SoundwelDatasetKey.csv",  # â† CSV-Pfad
    metadata=df,                                      # â† oder pre-loaded
    label_columns=["age", "sex", "valence", "context"],
    sample_rate=16000,
    max_length_sec=3.0,
    download=False,  # Local data
)
```

### 3. **Registry Cleanup**
- âœ… VoxMonitor registry.py nutzt nur noch DeepSuite HeadRegistry
- âœ… Keine Duplikation mehr
- âœ… Re-Export fÃ¼r Convenience-Imports

### 4. **Lightning Module Testing**
- âœ… VoxMonitorLightningModule vollstÃ¤ndig getestet
- âœ… Multi-Task Learning mit 4 Klassifikations-Zielen
- âœ… Per-Task Loss & Accuracy Tracking
- âœ… Hyperparameter Saving

---

## ğŸ¯ Lokale Daten-Struktur

```
data/
â”œâ”€â”€ audio/                       # Audio-Dateien (WAV)
â”‚   â”œâ”€â”€ sample1.wav
â”‚   â”œâ”€â”€ sample2.wav
â”‚   â””â”€â”€ ...
â”œâ”€â”€ SoundwelDatasetKey.csv       # Metadaten + Labels
â””â”€â”€ training/                    # Training-Artefakte
```

**CSV-Struktur:**
- Audio Filename: Dateiname (z.B. "ETHZETHZPositivePositivepig1510.wav")
- Age Category: Piglet, Weaner, Grower, Finisher
- Sex: male, female
- Valence: Pos, Neg, (Neutral)
- Context: Enriched, Barren, Isolation, etc.

---

## ğŸš€ Training starten

### Option 1: Quick 1-Epoch Test
```bash
cd /Users/anton.feldmann/Projects/priv/VoxMonitor
uv run python train_local.py
```

**Was passiert:**
1. Config wird erstellt (config_local.yaml)
2. CSV mit 3000+ Samples wird geladen
3. Dataset wird initialisiert
4. 1 Epoch Training startet
5. Metriken werden geloggt
6. ONNX-Export erfolgt automatisch

### Option 2: Mit Custom Config
```bash
uv run voxmonitor-train --config config/my_config.yaml
```

---

## ğŸ“Š Training-Parameter

```yaml
data:
  audio_dir: /Volumes/.../Soundwel
  csv_path: /Volumes/.../SoundwelDatasetKey.csv
  sample_rate: 16000
  max_length_sec: 3.0

train:
  batch_size: 16          # Increase fÃ¼r schneller Training
  max_epochs: 1           # Quick test
  lr: 1e-3
  weight_decay: 1e-5
  device: auto            # GPU/MPS/CPU
  checkpoint_dir: ckpt/soundwell_quick
  export_formats: [onnx]  # FÃ¼r OmniEngine
```

---

## ğŸ” Expected Output

```
======================================================================
ğŸš€ VoxMonitor Training - Local Soundwell Data (1 Epoch)
======================================================================

ğŸ“‚ Audio:    /Volumes/.../Soundwel
ğŸ“„ CSV:      /Volumes/.../SoundwelDatasetKey.csv

ğŸ“‹ Loading metadata...
   Total samples: 3000+

ğŸ”§ Creating dataset...
   âœ… 3000+ samples loaded

ğŸ·ï¸  Classes:
   age: 4 classes
   sex: 2 classes
   valence: 3 classes
   context: 5 classes

ğŸ“Š DataLoader: 188 batches of 16

ğŸ§  Creating Lightning module...
   Classes: {'age': 4, 'sex': 2, 'valence': 3, 'context': 5}

ğŸ‹ï¸  Creating trainer...
   âœ… Trainer ready

â±ï¸  Starting training...

----------------------------------------------------------------------
Epoch 1: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% Loss: 1.23
----------------------------------------------------------------------

âœ… Training completed successfully!

======================================================================
âœ¨ 1-epoch training test PASSED
======================================================================
```

---

## ğŸ“ˆ Output-Dateien

Nach dem Training:
```
ckpt/soundwell_quick/
â”œâ”€â”€ epoch-00-val_loss=0.xxx.ckpt    # PyTorch Lightning Checkpoint
â”œâ”€â”€ soundwell_final.pt                # PyTorch Model
â”œâ”€â”€ soundwell.onnx                    # ONNX Export (fÃ¼r OmniEngine!)
â””â”€â”€ training.log                      # Training-Logs
```

**ONNX-Modell fÃ¼r OmniEngine:**
- Input: Mel-Spektrogramm [1, 64, T]
- Output: Multi-Task Logits
  - age: [1, 4]
  - sex: [1, 2]
  - valence: [1, 3]
  - context: [1, 5]

---

## âœ… Checkliste vor dem Training

- [x] Data-Dateien vorhanden (/Volumes/Backup/.../Soundwell)
- [x] CSV-Metadaten verfÃ¼gbar (SoundwelDatasetKey.csv)
- [x] SoundwelDataset CSV-Support implementiert
- [x] VoxMonitorTrainer basiert auf DeepSuite BaseTrainer
- [x] VoxMonitorLightningModule multi-task fertig
- [x] Registry bereinigt (nur DeepSuite)
- [x] Training-Skript erstellt (train_local.py)
- [x] ONNX-Export konfiguriert

---

## ğŸ“ NÃ¤chste Schritte

1. **Test ausfÃ¼hren:** `uv run python train_local.py`
2. **ONNX validieren:** Modell mit OmniEngine laden
3. **Full Training:** Config anpassen fÃ¼r alle Samples
4. **Hyperparameter tuning:** Learning rate, Batch size, etc.
5. **Inference Pipeline:** VoxMonitor â†’ OmniEngine

---

## ğŸ“š Referenzen

- Soundwell Dataset: https://zenodo.org/records/8252482
- Paper: https://doi.org/10.1038/s41598-022-07174-8 (Briefer et al., 2022)
- DeepSuite: Multi-task learning framework
- OmniEngine: Production inference engine

---

**Status: READY TO TRAIN** âœ¨
